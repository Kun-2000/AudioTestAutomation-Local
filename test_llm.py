import sys
import logging
import asyncio
import json
from pathlib import Path

from services.llm_service import LLMService

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def run_llm_test():
    """
    ä¸€å€‹ç°¡å–®çš„å‡½å¼ï¼Œç”¨ä¾†åˆå§‹åŒ–ä¸¦åŸ·è¡Œ LLMServiceã€‚
    """
    print("\n" + "=" * 50)
    print("ğŸš€ é–‹å§‹ç¨ç«‹æ¸¬è©¦ LLMService (æœ¬åœ° Ollama)...")
    print("=" * 50)

    try:
        # --- æ­¥é©Ÿ 1: æº–å‚™æ¸¬è©¦ç”¨çš„æ–‡å­— ---
        print("\n[æ­¥é©Ÿ 1/3] æº–å‚™æ¸¬è©¦è…³æœ¬èˆ‡è½‰éŒ„æ–‡å­—...")
        original_script = (
            "å®¢æˆ¶: ä½ å¥½ï¼Œæˆ‘æƒ³æŸ¥è©¢æˆ‘çš„è¨‚å–®ç‹€æ…‹ã€‚\nå®¢æœ: å¥½çš„ï¼Œè«‹å•æ‚¨çš„è¨‚å–®ç·¨è™Ÿæ˜¯å¤šå°‘ï¼Ÿ"
        )
        transcribed_text = (
            "å®¢æˆ¶: ä½ å¥½æˆ‘æƒ³æŸ¥è©¢æˆ‘çš„è¨‚å–®ç‹€æ…‹\nå®¢æœ: å¥½çš„è«‹å•æ‚¨çš„è¨‚å–®ç·¨è™Ÿæ˜¯å¤šå°‘"
        )

        print("\nğŸ“œ åŸå§‹è…³æœ¬:")
        print(original_script)
        print("\nğŸ“ æ¨¡æ“¬è½‰éŒ„æ–‡å­—:")
        print(transcribed_text)

        # --- æ­¥é©Ÿ 2: åˆå§‹åŒ– LLMService ---
        print("\n[æ­¥é©Ÿ 2/3] æ­£åœ¨åˆå§‹åŒ– LLMService...")
        print("ï¼ˆè«‹ç¢ºä¿æ‚¨çš„æœ¬åœ° Ollama æœå‹™æ­£åœ¨é‹è¡Œï¼‰")
        llm_service = LLMService()
        print(f"âœ… LLMService åˆå§‹åŒ–æˆåŠŸï¼æ¨¡å‹: {llm_service.model}")

        # --- æ­¥é©Ÿ 3: åŸ·è¡Œå°è©±åˆ†æ ---
        print("\n[æ­¥é©Ÿ 3/3] æ­£åœ¨åŸ·è¡Œå°è©±åˆ†æ...")
        analysis_result = await llm_service.analyze_conversation(
            original_script, transcribed_text
        )

        print("\n" + "=" * 50)
        print("ğŸ‰ æ¸¬è©¦æˆåŠŸï¼")
        print("=" * 50)
        print("ğŸ¤– LLM åˆ†æå·²å®Œæˆï¼")

        # å°‡çµæœæ ¼å¼åŒ–ç‚ºæ˜“æ–¼é–±è®€çš„ JSON æ ¼å¼
        pretty_json = json.dumps(analysis_result, indent=2, ensure_ascii=False)
        print(pretty_json)

    except Exception as e:
        logging.error("âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: %s", e, exc_info=True)
        print("\n" + "=" * 50)
        print("ğŸ˜­ æ¸¬è©¦å¤±æ•—ã€‚è«‹æª¢æŸ¥ä¸Šé¢çš„éŒ¯èª¤è¨Šæ¯ã€‚")
        print("   å¯èƒ½çš„åŸå› ï¼š")
        print("   1. æœ¬åœ°çš„ Ollama æœå‹™æœªå•Ÿå‹•ã€‚")
        print("   2. .env æª”æ¡ˆä¸­çš„ LLM_API_BASE_URL æˆ– LLM_MODEL è¨­å®šä¸æ­£ç¢ºã€‚")
        print("=" * 50)


if __name__ == "__main__":
    asyncio.run(run_llm_test())
